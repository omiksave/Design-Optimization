{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# <h1> Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gradient - based optimization and differentiable programming"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pole - Cart Balancing Controller Design with Neural Network and Gradient Descent."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# overhead\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "M = 0.5\n",
    "m = 0.2\n",
    "b = 0.1\n",
    "I = 0.006\n",
    "g = 9.8\n",
    "l = 0.3\n",
    "p = I*(M+m)+M*m*(l**2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "A = t.tensor([[0,1,0,0],[0,-(I+m*(l**2))*b/p,((m**2)*g*(l**2))/p,0],\n",
    "              [0,0,0,1],[0,-(m*l*b)/p,m*g*l*(M+m)/p,0]]).float()\n",
    "B = t.tensor([[0],[(I+m*(l**2))/p],\n",
    "              [0],[m*l/p]]).float()\n",
    "C = t.tensor([[1,0,0,0],\n",
    "              [0,0,1,0]]).float()\n",
    "D = t.tensor([[0],[0]]).float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.0030])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.01*t.randn(size=[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class Dynamics(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dynamics, self).__init__()\n",
    "    @staticmethod\n",
    "    def forward(state,action):\n",
    "        dA = t.matmul(B,t.transpose(action,0,1))\n",
    "        Ac = A-dA\n",
    "        state = t.matmul(Ac,state)\n",
    "        state = state + 0.01*t.randn(size=[1])\n",
    "        return state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_input, dim_hidden, dim_output):\n",
    "        \"\"\"\n",
    "        dim_input: # of system states\n",
    "        dim_output: # of actions\n",
    "        dim_hidden: up to you\n",
    "        \"\"\"\n",
    "        super(Controller, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(dim_input, dim_hidden),\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear(dim_hidden, dim_hidden),\n",
    "            #nn.Sigmoid(),\n",
    "            #nn.Linear(dim_hidden, dim_hidden),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(dim_hidden, dim_output),\n",
    "            nn.Sigmoid()\n",
    "\n",
    "\n",
    "            # You can add more layers here\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        action = self.network(state)\n",
    "        return action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class Simulation(nn.Module):\n",
    "\n",
    "    def __init__(self, controller, dynamics, T):\n",
    "        super(Simulation, self).__init__()\n",
    "        self.state = self.initialize_state()\n",
    "        self.controller = controller\n",
    "        self.dynamics = dynamics\n",
    "        self.T = T\n",
    "        self.action_trajectory = []\n",
    "        self.state_trajectory = []\n",
    "\n",
    "    def forward(self, state):\n",
    "        self.action_trajectory = []\n",
    "        self.state_trajectory = []\n",
    "        for _ in range(T):\n",
    "            action = self.controller.forward(state)\n",
    "            state = self.dynamics.forward(state, action)\n",
    "            self.action_trajectory.append(action)\n",
    "            self.state_trajectory.append(state)\n",
    "        return self.error(state)\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_state():\n",
    "        state = [1, 0.,0.4,0.]  # TODO: need batch of initial states\n",
    "        return t.tensor(state,requires_grad=True).float()\n",
    "\n",
    "    def error(self, state):\n",
    "        return 0.1*state[0]**2 + 0.1*state[2]**0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class Optimize:\n",
    "    def __init__(self, simulation):\n",
    "        self.simulation = simulation\n",
    "        self.parameters = simulation.controller.parameters()\n",
    "        self.optimizer = optim.LBFGS(self.parameters, lr=0.01)\n",
    "\n",
    "    def step(self):\n",
    "        def closure():\n",
    "            loss = self.simulation(self.simulation.state)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        self.optimizer.step(closure)\n",
    "        return closure()\n",
    "\n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self.step()\n",
    "            print('[%d] loss: %.3f' % (epoch + 1, loss))\n",
    "            #self.visualize()\n",
    "\n",
    "    def visualize(self):\n",
    "        data = np.array([self.simulation.state_trajectory[i].detach().numpy() for i in range(self.simulation.T)])\n",
    "        x = data[:, 0]\n",
    "        y = data[:, 1]\n",
    "        plt.plot(x, y)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 69332560021601386496.000\n",
      "[2] loss: 66214995956112293888.000\n",
      "[3] loss: 64326509968616325120.000\n",
      "[4] loss: 63339412409664143360.000\n",
      "[5] loss: 62826727729818238976.000\n",
      "[6] loss: 62519933199343157248.000\n",
      "[7] loss: 62311008397879672832.000\n",
      "[8] loss: 62128911680133922816.000\n",
      "[9] loss: 62022624090100072448.000\n",
      "[10] loss: 61956618208061423616.000\n",
      "[11] loss: 61914638854112935936.000\n",
      "[12] loss: 61888725564069511168.000\n",
      "[13] loss: 61871093795606495232.000\n",
      "[14] loss: 61859680864910180352.000\n",
      "[15] loss: 61852907873283080192.000\n",
      "[16] loss: 61851636837841371136.000\n",
      "[17] loss: 61848435059981287424.000\n",
      "[18] loss: 61847067267516334080.000\n",
      "[19] loss: 61846082105097846784.000\n",
      "[20] loss: 61845611514121158656.000\n",
      "[21] loss: 61845365223516536832.000\n",
      "[22] loss: 61845365223516536832.000\n",
      "[23] loss: 61845220087981670400.000\n",
      "[24] loss: 61845184903609581568.000\n",
      "[25] loss: 61844978195423559680.000\n",
      "[26] loss: 61844956205191004160.000\n",
      "[27] loss: 61844727506772426752.000\n",
      "[28] loss: 61844727506772426752.000\n",
      "[29] loss: 61844516400539893760.000\n",
      "[30] loss: 61844516400539893760.000\n",
      "[31] loss: 61844516400539893760.000\n",
      "[32] loss: 61844516400539893760.000\n",
      "[33] loss: 61844331682586427392.000\n",
      "[34] loss: 61844331682586427392.000\n",
      "[35] loss: 61844331682586427392.000\n",
      "[36] loss: 61844331682586427392.000\n",
      "[37] loss: 61844331682586427392.000\n",
      "[38] loss: 61844270109935271936.000\n",
      "[39] loss: 61844155760725983232.000\n",
      "[40] loss: 61844155760725983232.000\n"
     ]
    }
   ],
   "source": [
    "#frame = 0.2\n",
    "dim_input = 4\n",
    "dim_output = 1\n",
    "dim_hidden = 10\n",
    "T = 500\n",
    "d = Dynamics()  # define dynamics\n",
    "c = Controller(dim_input, dim_hidden, dim_output)  # define controller\n",
    "s = Simulation(c, d, T)  # define simulation\n",
    "o = Optimize(s)  # define optimizer\n",
    "o.train(40)  #"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.0082], grad_fn=<SigmoidBackward>)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.forward(t.tensor([0,0.,0.2,0.]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([0.0001], grad_fn=<SigmoidBackward>),\n tensor([9.9939e-05], grad_fn=<SigmoidBackward>),\n tensor([7.8300e-05], grad_fn=<SigmoidBackward>),\n tensor([6.1028e-05], grad_fn=<SigmoidBackward>),\n tensor([4.7284e-05], grad_fn=<SigmoidBackward>),\n tensor([3.6388e-05], grad_fn=<SigmoidBackward>),\n tensor([2.7760e-05], grad_fn=<SigmoidBackward>),\n tensor([2.0999e-05], grad_fn=<SigmoidBackward>),\n tensor([1.5737e-05], grad_fn=<SigmoidBackward>),\n tensor([1.1673e-05], grad_fn=<SigmoidBackward>),\n tensor([8.5631e-06], grad_fn=<SigmoidBackward>),\n tensor([6.2057e-06], grad_fn=<SigmoidBackward>),\n tensor([4.4386e-06], grad_fn=<SigmoidBackward>),\n tensor([3.1299e-06], grad_fn=<SigmoidBackward>),\n tensor([2.1736e-06], grad_fn=<SigmoidBackward>),\n tensor([1.4848e-06], grad_fn=<SigmoidBackward>),\n tensor([9.9661e-07], grad_fn=<SigmoidBackward>),\n tensor([6.5653e-07], grad_fn=<SigmoidBackward>),\n tensor([4.2405e-07], grad_fn=<SigmoidBackward>),\n tensor([2.6805e-07], grad_fn=<SigmoidBackward>),\n tensor([1.6560e-07], grad_fn=<SigmoidBackward>),\n tensor([9.9824e-08], grad_fn=<SigmoidBackward>),\n tensor([5.8627e-08], grad_fn=<SigmoidBackward>),\n tensor([3.3489e-08], grad_fn=<SigmoidBackward>),\n tensor([1.8573e-08], grad_fn=<SigmoidBackward>),\n tensor([9.9827e-09], grad_fn=<SigmoidBackward>),\n tensor([5.1897e-09], grad_fn=<SigmoidBackward>),\n tensor([2.6041e-09], grad_fn=<SigmoidBackward>),\n tensor([1.2586e-09], grad_fn=<SigmoidBackward>),\n tensor([5.8454e-10], grad_fn=<SigmoidBackward>),\n tensor([2.6025e-10], grad_fn=<SigmoidBackward>),\n tensor([1.1079e-10], grad_fn=<SigmoidBackward>),\n tensor([4.4981e-11], grad_fn=<SigmoidBackward>),\n tensor([1.7366e-11], grad_fn=<SigmoidBackward>),\n tensor([6.3565e-12], grad_fn=<SigmoidBackward>),\n tensor([2.1990e-12], grad_fn=<SigmoidBackward>),\n tensor([7.1657e-13], grad_fn=<SigmoidBackward>),\n tensor([2.1918e-13], grad_fn=<SigmoidBackward>),\n tensor([6.2691e-14], grad_fn=<SigmoidBackward>),\n tensor([1.6703e-14], grad_fn=<SigmoidBackward>),\n tensor([4.1282e-15], grad_fn=<SigmoidBackward>),\n tensor([9.4226e-16], grad_fn=<SigmoidBackward>),\n tensor([1.9771e-16], grad_fn=<SigmoidBackward>),\n tensor([3.7948e-17], grad_fn=<SigmoidBackward>),\n tensor([6.6283e-18], grad_fn=<SigmoidBackward>),\n tensor([1.0478e-18], grad_fn=<SigmoidBackward>),\n tensor([1.4904e-19], grad_fn=<SigmoidBackward>),\n tensor([1.8960e-20], grad_fn=<SigmoidBackward>),\n tensor([2.1430e-21], grad_fn=<SigmoidBackward>),\n tensor([2.1375e-22], grad_fn=<SigmoidBackward>),\n tensor([1.8677e-23], grad_fn=<SigmoidBackward>),\n tensor([1.4189e-24], grad_fn=<SigmoidBackward>),\n tensor([9.2955e-26], grad_fn=<SigmoidBackward>),\n tensor([5.2067e-27], grad_fn=<SigmoidBackward>),\n tensor([2.4711e-28], grad_fn=<SigmoidBackward>),\n tensor([9.8419e-30], grad_fn=<SigmoidBackward>),\n tensor([3.2563e-31], grad_fn=<SigmoidBackward>),\n tensor([8.8551e-33], grad_fn=<SigmoidBackward>),\n tensor([1.9568e-34], grad_fn=<SigmoidBackward>),\n tensor([3.4718e-36], grad_fn=<SigmoidBackward>),\n tensor([4.8837e-38], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>),\n tensor([0.], grad_fn=<SigmoidBackward>)]"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.action_trajectory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}